{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deals with everything that reads and write to the s3 cache for the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pickle, os\n",
    "\n",
    "os.environ['DATABASE_TABLE_NAME'] = 'product-table-dev-manual'\n",
    "os.environ['REGION'] = 'ap-southeast-1'\n",
    "os.environ['INVENTORY_BUCKET_NAME'] = 'product-bucket-dev-manual'\n",
    "os.environ['INPUT_BUCKET_NAME'] = 'input-product-bucket-dev-manual'\n",
    "os.environ['DAX_ENDPOINT'] = 'longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111'\n",
    "os.environ['LINEKEY'] = '2uAfV4AoYglUGmKTAk2xNOm0aV2Ufgh1BQPvQl9vJd4'\n",
    "REGION = 'ap-southeast-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from s3bz.s3bz import S3\n",
    "from nicHelper.wrappers import add_method, add_class_method, add_static_method\n",
    "from nicHelper.dictUtil import stripDict, printDict, hashDict, saveStringToFile, loadStringFromFile, saveDictToFile, loadDictFromFile\n",
    "from nicHelper.exception import errorString\n",
    "import nicHelper.pdUtils as pdUtils\n",
    "from dict_hash import dict_hash, sha256\n",
    "from base64 import b64encode, b64decode\n",
    "from pandas.util import hash_pandas_object\n",
    "from hashlib import sha1\n",
    "import pandas as pd\n",
    "import os, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n",
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n",
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n",
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n"
     ]
    }
   ],
   "source": [
    "from villaProductDatabase.database import ProductDatabase\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import os\n",
    "DBHASHLOCATION = '/mnt/efs/database.hash'\n",
    "DBCACHELOCATION = '/mnt/efs/database.cache'\n",
    "DATABASE_TABLE_NAME = os.environ.get('DATABASE_TABLE_NAME')\n",
    "INVENTORY_BUCKET_NAME = os.environ.get('INVENTORY_BUCKET_NAME')\n",
    "INPUT_BUCKET_NAME = os.environ.get('INPUT_BUCKET_NAME')\n",
    "REGION = os.environ.get('REGION') or 'ap-southeast-1'\n",
    "ACCESS_KEY_ID = os.environ.get('USER') or None\n",
    "SECRET_ACCESS_KEY = os.environ.get('PW') or None\n",
    "LINEKEY= os.environ.get('LINEKEY')\n",
    "ALLDATAKEY = 'allData'\n",
    "  \n",
    "try:\n",
    "  DAX_ENDPOINT = os.environ['DAX_ENDPOINT']\n",
    "  print(DAX_ENDPOINT)\n",
    "except KeyError as e:\n",
    "  print(f'dax endpoint missing {e}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class S3Cache:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester( S3Cache, ProductDatabase):\n",
    "  class Meta:\n",
    "    table_name = os.environ['DATABASE_TABLE_NAME']\n",
    "    region = os.environ['REGION']\n",
    "    billing_mode='PAY_PER_REQUEST'\n",
    "    dax_read_endpoints = [DAX_ENDPOINT] if DAX_ENDPOINT else None\n",
    "    dax_write_endpoints = [DAX_ENDPOINT] if DAX_ENDPOINT else None\n",
    "  pass\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save and load local hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3Cache)\n",
    "def loadFromCache(cls, key=ALLDATAKEY, localCache=DBCACHELOCATION, \n",
    "                  localHash=DBHASHLOCATION, bucket=INVENTORY_BUCKET_NAME):\n",
    "  ## check for local object and its hash\n",
    "  if os.path.exists(localCache) and os.path.exists(localHash):\n",
    "    try:\n",
    "      localHash = pdUtils.loadLocalHash(path=localHash)\n",
    "      logging.debug(f'localHash is {localHash}')\n",
    "      remoteHash = pdUtils.loadRemoteHash(key=key, bucket=bucket)\n",
    "      logging.debug(f'remoteHash is {remoteHash}')\n",
    "\n",
    "      if localHash == remoteHash:\n",
    "        print('data is still in sync, using local cache')\n",
    "        db = pdUtils.loadLocalCache(path=localCache)\n",
    "        return db\n",
    "      else: \n",
    "        print('remote hash is not the same, load remote cache')\n",
    "    except Exception as e: print(f'local loading error{e}, loading remote hash')\n",
    "  ### load from remote cache\n",
    "  try:\n",
    "    db = pdUtils.loadRemoteCache(key=key, bucket=bucket)\n",
    "    pdUtils.saveLocalCache(db, path=localCache)\n",
    "    pdUtils.saveLocalHash(db, path = localHash)\n",
    "  except Exception as e:\n",
    "    print(f'locding remtoe failed {e} returning blank df')\n",
    "    db = pd.DataFrame()\n",
    "  ### save to local cache\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading hashkey allData-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded hash is PzfrumW4Vib/5yh3/4UtOHZI88U=\n",
      "remote hash is not the same, load remote cache\n",
      "CPU times: user 1.2 s, sys: 257 ms, total: 1.46 s\n",
      "Wall time: 2.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db = Tester.loadFromCache()\n",
    "assert db.shape > (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locding remtoe failed An error occurred (404) when calling the HeadObject operation: Not Found returning blank df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tester.loadFromCache(key='doestExist', localCache='doesntExist', localHash='doesntExist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load remote hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@add_class_method(S3Cache)\n",
    "def saveRemoteCache(cls ,db:pd.DataFrame, key= ALLDATAKEY,\n",
    "                   bucket = INVENTORY_BUCKET_NAME, localCachePath=DBCACHELOCATION,\n",
    "                   localHashPath=DBHASHLOCATION):\n",
    "  pdUtils.saveRemoteCache(data=db, key= key, \n",
    "                          bucket=bucket, localCachePath=localCachePath, \n",
    "                          localHashPath=localHashPath)\n",
    "  jsonDb = db.to_json(orient='split')\n",
    "  zlibArc = zlib.compress(jsonDb.encode())\n",
    "  tmpPath = '/tmp/zlibJsonCache.zl'\n",
    "  with open(tmpPath, 'wb') as f:\n",
    "    f.write(zlibArc)\n",
    "  S3.saveFile(key=f'{key}-json.zl',path=tmpPath,bucket=bucket)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:data was saved to s3\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashKey is allData-hash\n",
      "saving hash to s3\n",
      "saved hash e706d6d6b6a9a7e831045ed1e859e7fb24bd5e4b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading hashkey allData-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded hash is e706d6d6b6a9a7e831045ed1e859e7fb24bd5e4b\n",
      "data is still in sync, using local cache\n",
      "CPU times: user 3.92 s, sys: 457 ms, total: 4.38 s\n",
      "Wall time: 5.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Tester.saveRemoteCache(db)\n",
    "assert pdUtils.getDfHash(db) == pdUtils.getDfHash(Tester.loadFromCache())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset S3 Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3Cache)\n",
    "def resetS3Cache(cls, bucketName= INVENTORY_BUCKET_NAME, key = 'allData', limit=10000, **kwargs):\n",
    "  ''' upload changes to s3'''\n",
    "  ###### get all data\n",
    "  items:List[cls] = cls.scanDb(limit=limit)\n",
    "  db:pd.DataFrame = cls.toDf(items)\n",
    "  print(f'{db.shape} changes to update')\n",
    "  cls.saveRemoteCache(db)\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Tester.resetS3Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDb = db.to_json(orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 379 ms, sys: 39.4 ms, total: 418 ms\n",
      "Wall time: 417 ms\n",
      "CPU times: user 1.41 s, sys: 14.7 ms, total: 1.42 s\n",
      "Wall time: 1.46 s\n",
      "CPU times: user 126 ms, sys: 63.3 ms, total: 189 ms\n",
      "Wall time: 189 ms\n",
      "CPU times: user 848 ms, sys: 74.8 ms, total: 923 ms\n",
      "Wall time: 920 ms\n"
     ]
    }
   ],
   "source": [
    "import zlib, json\n",
    "%time jsonDb = db.to_json(orient='split')\n",
    "%time zlibArc = zlib.compress(jsonDb.encode())\n",
    "%time zlibObject = zlib.decompress(zlibArc).decode()\n",
    "%time dbDict = json.loads(zlibObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45878793\n",
      "6645115\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(jsonDb))\n",
    "print(sys.getsizeof(zlibArc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
