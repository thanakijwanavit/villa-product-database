{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deals with everything that reads and write to the s3 cache for the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from s3bz.s3bz import S3\n",
    "from nicHelper.wrappers import add_method, add_class_method, add_static_method\n",
    "from nicHelper.dictUtil import stripDict, printDict, hashDict, saveStringToFile, loadStringFromFile, saveDictToFile, loadDictFromFile\n",
    "from nicHelper.exception import errorString\n",
    "from dict_hash import dict_hash, sha256\n",
    "from base64 import b64encode, b64decode\n",
    "import os, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pickle, os\n",
    "\n",
    "os.environ['DATABASE_TABLE_NAME'] = 'product-table-dev-manual'\n",
    "os.environ['REGION'] = 'ap-southeast-1'\n",
    "os.environ['INVENTORY_BUCKET_NAME'] = 'product-bucket-dev-manual'\n",
    "os.environ['INPUT_BUCKET_NAME'] = 'input-product-bucket-dev-manual'\n",
    "os.environ['DAX_ENDPOINT'] = 'longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111'\n",
    "os.environ['LINEKEY'] = '2uAfV4AoYglUGmKTAk2xNOm0aV2Ufgh1BQPvQl9vJd4'\n",
    "REGION = 'ap-southeast-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from villaProductDatabase.database import ProductDatabase\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import os\n",
    "DBHASHLOCATION = '/tmp/database.hash'\n",
    "DBCACHELOCATION = '/tmp/database.cache'\n",
    "DATABASE_TABLE_NAME = os.environ.get('DATABASE_TABLE_NAME')\n",
    "INVENTORY_BUCKET_NAME = os.environ.get('INVENTORY_BUCKET_NAME')\n",
    "INPUT_BUCKET_NAME = os.environ.get('INPUT_BUCKET_NAME')\n",
    "REGION = os.environ.get('REGION') or 'ap-southeast-1'\n",
    "ACCESS_KEY_ID = os.environ.get('USER') or None\n",
    "SECRET_ACCESS_KEY = os.environ.get('PW') or None\n",
    "LINEKEY= os.environ.get('LINEKEY')\n",
    "  \n",
    "try:\n",
    "  DAX_ENDPOINT = os.environ['DAX_ENDPOINT']\n",
    "  print(DAX_ENDPOINT)\n",
    "except KeyError as e:\n",
    "  print(f'dax endpoint missing {e}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class S3Cache:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester( S3Cache, ProductDatabase):\n",
    "  class Meta:\n",
    "    table_name = os.environ['DATABASE_TABLE_NAME']\n",
    "    region = os.environ['REGION']\n",
    "    billing_mode='PAY_PER_REQUEST'\n",
    "    dax_read_endpoints = [DAX_ENDPOINT] if DAX_ENDPOINT else None\n",
    "    dax_write_endpoints = [DAX_ENDPOINT] if DAX_ENDPOINT else None\n",
    "  pass\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3Cache)\n",
    "def saveHash(cls , data:dict, key='allData', bucket=INVENTORY_BUCKET_NAME, \n",
    "             cachePath=DBCACHELOCATION, hashPath = DBHASHLOCATION):\n",
    "  hashKey = f'{key}-hash'\n",
    "  hashString = hashDict(data)\n",
    "  dictToSave= {'hash': hashString }\n",
    "  print(f'hashKey is {hashKey}')\n",
    "  print(f'saving cache file')\n",
    "  saveDictToFile(data, path = cachePath)\n",
    "  print(f'saving hash file')\n",
    "  saveStringToFile(hashString, path=hashPath)\n",
    "  print('saving hash to s3')\n",
    "  S3.save(key=hashKey,objectToSave=dictToSave, bucket=bucket)\n",
    "  print(f'saved hash {hashString}')\n",
    "@add_class_method(S3Cache)\n",
    "def loadHash(cls,key='allData', bucket=INVENTORY_BUCKET_NAME):\n",
    "  hashKey = f'{key}-hash'\n",
    "  print(f'loading hashkey {hashKey}')\n",
    "  loadedHash= S3.load(hashKey,bucket=bucket).get('hash')\n",
    "  print(f'loaded hash is{loadedHash}')\n",
    "  return loadedHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:data was saved to s3\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashKey is testHash-hash\n",
      "saving cache file\n",
      "saving hash file\n",
      "saving hash to s3\n",
      "saved hash 9Sx/tDiQ9yHHG6sI8HBk1huYVbk=\n",
      "loading hashkey testHash-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded hash is9Sx/tDiQ9yHHG6sI8HBk1huYVbk=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9Sx/tDiQ9yHHG6sI8HBk1huYVbk='"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testKey = 'testHash'\n",
    "S3Cache.saveHash({'test':'test'}, key=testKey)\n",
    "S3Cache.loadHash(key=testKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3Cache)\n",
    "def loadFromS3(cls, bucketName= INVENTORY_BUCKET_NAME, key = 'allData',\n",
    "               hashPath=DBHASHLOCATION, cachePath = DBCACHELOCATION,**kwargs):\n",
    "  '''\n",
    "  this is not a real time function, there may be a delay of sync between\n",
    "  the main dynamodb database and the cache\n",
    "  '''\n",
    "  \n",
    "  if os.path.exists(hashPath) and os.path.exists(cachePath):\n",
    "    print('cache exist')\n",
    "    if cls.loadHash(key=key) == loadStringFromFile(hashPath):\n",
    "      db = loadDictFromFile(cachePath)\n",
    "      print('found a valid cache, using cache')\n",
    "      return db\n",
    "    else:\n",
    "      print('cache has different hash than s3')\n",
    "  print('cache doesnt exist')\n",
    "  logging.info(f'loading from {bucketName}')\n",
    "  logging.info(f'user is {kwargs.get(\"user\")}')\n",
    "  database =  S3.loadPklZl(key=f'{key}-pklzl', bucket = bucketName,  **kwargs)\n",
    "#   database =  S3.load(key=f'{key}', bucket = bucketName,  **kwargs)\n",
    "  print(database)\n",
    "  cls.saveHash(database)\n",
    "  return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading hashkey allData-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded hash isVwSSjMiQzlnwM4SuIQumrYVPJTk=\n",
      "CPU times: user 51.4 ms, sys: 0 ns, total: 51.4 ms\n",
      "Wall time: 489 ms\n",
      "CPU times: user 306 µs, sys: 0 ns, total: 306 µs\n",
      "Wall time: 234 µs\n"
     ]
    }
   ],
   "source": [
    "%time Tester.loadHash(key='allData')\n",
    "%time db = loadDictFromFile(DBCACHELOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from s3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 632 ms, sys: 33.8 ms, total: 666 ms\n",
      "Wall time: 1.16 s\n",
      "45149\n",
      "filter db for pandas\n",
      "CPU times: user 65.9 ms, sys: 0 ns, total: 65.9 ms\n",
      "Wall time: 65.6 ms\n",
      "gen pandas df\n",
      "CPU times: user 1.36 s, sys: 0 ns, total: 1.36 s\n",
      "Wall time: 1.36 s\n",
      "csv\n",
      "CPU times: user 947 ms, sys: 2.53 ms, total: 950 ms\n",
      "Wall time: 962 ms\n",
      "CPU times: user 747 ms, sys: 30.6 ms, total: 778 ms\n",
      "Wall time: 777 ms\n",
      "31.603618\n",
      "parquet\n",
      "CPU times: user 637 ms, sys: 27.9 ms, total: 664 ms\n",
      "Wall time: 663 ms\n",
      "CPU times: user 341 ms, sys: 92.4 ms, total: 433 ms\n",
      "Wall time: 349 ms\n",
      "17.107349\n",
      "pickle\n",
      "CPU times: user 663 ms, sys: 87.9 ms, total: 751 ms\n",
      "Wall time: 762 ms\n",
      "CPU times: user 227 ms, sys: 103 ms, total: 330 ms\n",
      "Wall time: 328 ms\n",
      "37.249584\n",
      "feather\n",
      "CPU times: user 397 ms, sys: 55.2 ms, total: 452 ms\n",
      "Wall time: 405 ms\n",
      "CPU times: user 200 ms, sys: 67.1 ms, total: 267 ms\n",
      "Wall time: 254 ms\n",
      "23.96149\n",
      "hdf\n",
      "CPU times: user 727 ms, sys: 136 ms, total: 864 ms\n",
      "Wall time: 880 ms\n",
      "CPU times: user 615 ms, sys: 219 ms, total: 834 ms\n",
      "Wall time: 1.23 s\n",
      "36.20258\n"
     ]
    }
   ],
   "source": [
    "key = 'allData'\n",
    "tempPath = '/tmp/test.csv'\n",
    "print('load from s3')\n",
    "%time database =  S3.loadPklZl(key=f'{key}-pklzl', bucket=INVENTORY_BUCKET_NAME)\n",
    "print(len(database))\n",
    "print('filter db for pandas')\n",
    "%time filteredDb = [list(item.values())[0] for item in database.values()]\n",
    "print('gen pandas df')\n",
    "%time df = pd.DataFrame(filteredDb).astype(str)\n",
    "\n",
    "print('csv')\n",
    "%time df.to_csv(tempPath)\n",
    "%time pd.read_csv(tempPath, dtype=str)\n",
    "print(os.path.getsize(tempPath)/1e6)\n",
    "print('parquet')\n",
    "%time df.to_parquet(tempPath)\n",
    "%time pd.read_parquet(tempPath)\n",
    "print(os.path.getsize(tempPath)/1e6)\n",
    "print('pickle')\n",
    "%time df.to_pickle(tempPath)\n",
    "%time pd.read_pickle(tempPath)\n",
    "print(os.path.getsize(tempPath)/1e6)\n",
    "print('feather')\n",
    "%time df.to_feather(tempPath)\n",
    "%time pd.read_feather(tempPath)\n",
    "print(os.path.getsize(tempPath)/1e6)\n",
    "print('hdf')\n",
    "os.remove(tempPath)\n",
    "%time df.to_hdf(tempPath,key='df')\n",
    "%time pd.read_hdf(tempPath, key='df')\n",
    "print(os.path.getsize(tempPath)/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hello</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hello world\n",
       "0     1     1\n",
       "1     2     2\n",
       "2     2     3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "df = pd.DataFrame({'hello':['1','2','2'], 'world':['1','2','3']})\n",
    "df\n",
    "# tableDict = df.to_dict(orient='list')\n",
    "# json.dumps(tableDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to s3 with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3Cache)\n",
    "def saveAllS3(cls, objectToSave:dict, bucketName= INVENTORY_BUCKET_NAME, key = 'allData', \n",
    "              hashPath = DBHASHLOCATION, cachePath = DBCACHELOCATION, **kwargs):\n",
    "  if os.path.exists(cachePath) and os.path.exists(hashPath):\n",
    "    if loadStringFromFile(hashPath) == cls.loadHash(key=key, bucket=bucketName):\n",
    "      print('the object did not change, skip saving')\n",
    "      return\n",
    "  S3.save(key=key, bucket=bucketName, objectToSave=objectToSave)\n",
    "  S3.savePklZl(key=f'{key}-pklzl',bucket=bucketName, objectToSave=objectToSave)\n",
    "  S3.saveZl(key=f'{key}-zl',bucket=bucketName, objectToSave=objectToSave)\n",
    "  print(f'saving hash with key {key}')\n",
    "  cls.saveHash(objectToSave, key=key)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading hashkey testKey-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded hash isx8KAJ3w/W83jHPrsnTFSxm3egUc=\n",
      "the object did not change, skip saving\n",
      "cache exist\n",
      "loading hashkey testKey-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded hash isx8KAJ3w/W83jHPrsnTFSxm3egUc=\n",
      "found a valid cache, using cache\n",
      "CPU times: user 97.1 ms, sys: 1.5 ms, total: 98.6 ms\n",
      "Wall time: 854 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test': 'test'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "key = 'testKey'\n",
    "Tester.saveAllS3(objectToSave={'test':'test'}, key = key)\n",
    "Tester.loadFromS3(key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache exist\n",
      "hashKey is allData-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found a valid cache, using cache\n",
      "CPU times: user 59.3 ms, sys: 3.86 ms, total: 63.2 ms\n",
      "Wall time: 488 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import sys\n",
    "database = Tester.loadFromS3()\n",
    "sys.getsizeof(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.5 ms, sys: 0 ns, total: 2.5 ms\n",
      "Wall time: 2.79 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import msgpack\n",
    "with open ('/tmp/testfile.test', 'wb') as f:\n",
    "  data = msgpack.packb(database)\n",
    "  f.write(data)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 823 µs, sys: 0 ns, total: 823 µs\n",
      "Wall time: 498 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open ('/tmp/testfile.test', 'rb') as f:\n",
    "  data = f.read()\n",
    "  msgpack.unpackb(data)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 875 µs, sys: 161 µs, total: 1.04 ms\n",
      "Wall time: 701 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open ('/tmp/testfilep.test', 'wb') as f:\n",
    "  pickle.dump(database,f)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 754 µs, sys: 0 ns, total: 754 µs\n",
      "Wall time: 548 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open ('/tmp/testfilep.test', 'rb') as f:\n",
    "  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-96fa5e73cad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilteredDb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilteredDb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# %time df.set_index('iprcode')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-96fa5e73cad9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilteredDb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilteredDb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# %time df.set_index('iprcode')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "filteredDb = [list(item.values())[0] for item in database.values()]\n",
    "df:pd.DataFrame = pd.DataFrame(filteredDb)\n",
    "df.head()\n",
    "# %time df.set_index('iprcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time res = df.to_json()\n",
    "%time res = df.to_csv('/tmp/test.csv')\n",
    "%time df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(database )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nicHelper.dictUtil import hashDict\n",
    "%time hashDict(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3.save(key='test',objectToSave=hashDict(database), bucket=INVENTORY_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
