{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deals with everything that reads and write to the s3 cache for the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pickle, os\n",
    "\n",
    "os.environ['DATABASE_TABLE_NAME'] = 'product-table-dev-manual'\n",
    "os.environ['REGION'] = 'ap-southeast-1'\n",
    "os.environ['INVENTORY_BUCKET_NAME'] = 'product-bucket-dev-manual'\n",
    "os.environ['INPUT_BUCKET_NAME'] = 'input-product-bucket-dev-manual'\n",
    "os.environ['DAX_ENDPOINT'] = 'longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111'\n",
    "os.environ['LINEKEY'] = '2uAfV4AoYglUGmKTAk2xNOm0aV2Ufgh1BQPvQl9vJd4'\n",
    "REGION = 'ap-southeast-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from s3bz.s3bz import S3\n",
    "from nicHelper.wrappers import add_method, add_class_method, add_static_method\n",
    "from nicHelper.dictUtil import stripDict, printDict, hashDict, saveStringToFile, loadStringFromFile, saveDictToFile, loadDictFromFile\n",
    "from nicHelper.exception import errorString\n",
    "import nicHelper.pdUtils as pdUtils\n",
    "from dict_hash import dict_hash, sha256\n",
    "from base64 import b64encode, b64decode\n",
    "from pandas.util import hash_pandas_object\n",
    "from hashlib import sha1\n",
    "import pandas as pd\n",
    "import os, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n",
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n",
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n"
     ]
    }
   ],
   "source": [
    "from villaProductDatabase.database import ProductDatabase\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import os\n",
    "DBHASHLOCATION = '/mnt/efs/database.hash'\n",
    "DBCACHELOCATION = '/mnt/efs/database.cache'\n",
    "DATABASE_TABLE_NAME = os.environ.get('DATABASE_TABLE_NAME')\n",
    "INVENTORY_BUCKET_NAME = os.environ.get('INVENTORY_BUCKET_NAME')\n",
    "INPUT_BUCKET_NAME = os.environ.get('INPUT_BUCKET_NAME')\n",
    "REGION = os.environ.get('REGION') or 'ap-southeast-1'\n",
    "ACCESS_KEY_ID = os.environ.get('USER') or None\n",
    "SECRET_ACCESS_KEY = os.environ.get('PW') or None\n",
    "LINEKEY= os.environ.get('LINEKEY')\n",
    "ALLDATAKEY = 'allData'\n",
    "  \n",
    "try:\n",
    "  DAX_ENDPOINT = os.environ['DAX_ENDPOINT']\n",
    "  print(DAX_ENDPOINT)\n",
    "except KeyError as e:\n",
    "  print(f'dax endpoint missing {e}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class S3Cache:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester( S3Cache, ProductDatabase):\n",
    "  class Meta:\n",
    "    table_name = os.environ['DATABASE_TABLE_NAME']\n",
    "    region = os.environ['REGION']\n",
    "    billing_mode='PAY_PER_REQUEST'\n",
    "    dax_read_endpoints = [DAX_ENDPOINT] if DAX_ENDPOINT else None\n",
    "    dax_write_endpoints = [DAX_ENDPOINT] if DAX_ENDPOINT else None\n",
    "  pass\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save and load local hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3Cache)\n",
    "def loadFromCache(cls, key=ALLDATAKEY, localCache=DBCACHELOCATION, \n",
    "                  localHash=DBHASHLOCATION, bucket=INVENTORY_BUCKET_NAME):\n",
    "  ## check for local object and its hash\n",
    "  if os.path.exists(localCache) and os.path.exists(localHash):\n",
    "    localHash = pdUtils.loadLocalHash(path=localHash)\n",
    "    remoteHash = pdUtils.loadRemoteHash(key=key, bucket=bucket)\n",
    "    if localHash == remoteHash:\n",
    "      print('data is still in sync, using local cache')\n",
    "      db = pdUtils.loadLocalCache(path=localCache)\n",
    "      return db\n",
    "    else: \n",
    "      print('remote hash is not the same, load remote cache')\n",
    "  ### load from remote cache\n",
    "  db = pdUtils.loadRemoteCache(key=key, bucket=bucket)\n",
    "  ### save to local cache\n",
    "  pdUtils.saveLocalCache(db, path=localCache)\n",
    "  pdUtils.saveLocalHash(db, path = localHash)\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading hashkey allData-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded hash is e706d6d6b6a9a7e831045ed1e859e7fb24bd5e4b\n",
      "data is still in sync, using local cache\n",
      "CPU times: user 315 ms, sys: 36.8 ms, total: 351 ms\n",
      "Wall time: 746 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db = Tester.loadFromCache()\n",
    "assert db.shape > (10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load remote hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@add_class_method(S3Cache)\n",
    "def saveRemoteCache(cls ,db:pd.DataFrame, key= ALLDATAKEY,\n",
    "                   bucket = INVENTORY_BUCKET_NAME, localCachePath=DBCACHELOCATION,\n",
    "                   localHashPath=DBHASHLOCATION):\n",
    "  pdUtils.saveRemoteCache(data=db, key= key, \n",
    "                          bucket=bucket, localCachePath=localCachePath, \n",
    "                          localHashPath=localHashPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:data was saved to s3\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashKey is allData-hash\n",
      "saving hash to s3\n",
      "saved hash e706d6d6b6a9a7e831045ed1e859e7fb24bd5e4b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading hashkey allData-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded hash is e706d6d6b6a9a7e831045ed1e859e7fb24bd5e4b\n",
      "data is still in sync, using local cache\n",
      "CPU times: user 2.1 s, sys: 439 ms, total: 2.54 s\n",
      "Wall time: 3.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Tester.saveRemoteCache(db)\n",
    "assert pdUtils.getDfHash(db) == pdUtils.getDfHash(Tester.loadFromCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
