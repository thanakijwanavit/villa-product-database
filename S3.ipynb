{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deals with everything that reads and write to the s3 cache for the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from s3bz.s3bz import S3\n",
    "from nicHelper.wrappers import add_method, add_class_method, add_static_method\n",
    "from nicHelper.dictUtil import stripDict, printDict, hashDict, saveStringToFile, loadStringFromFile, saveDictToFile, loadDictFromFile\n",
    "from nicHelper.exception import errorString\n",
    "from dict_hash import dict_hash, sha256\n",
    "from base64 import b64encode, b64decode\n",
    "import os, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pickle, os\n",
    "\n",
    "os.environ['DATABASE_TABLE_NAME'] = 'product-table-dev-manual'\n",
    "os.environ['REGION'] = 'ap-southeast-1'\n",
    "os.environ['INVENTORY_BUCKET_NAME'] = 'product-bucket-dev-manual'\n",
    "os.environ['INPUT_BUCKET_NAME'] = 'input-product-bucket-dev-manual'\n",
    "os.environ['DAX_ENDPOINT'] = 'longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111'\n",
    "os.environ['LINEKEY'] = '2uAfV4AoYglUGmKTAk2xNOm0aV2Ufgh1BQPvQl9vJd4'\n",
    "REGION = 'ap-southeast-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n",
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n",
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n"
     ]
    }
   ],
   "source": [
    "from villaProductDatabase.database import ProductDatabase\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longtermcluster.vuu7lr.clustercfg.dax.apse1.cache.amazonaws.com:8111\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "import os\n",
    "DBHASHLOCATION = '/tmp/database.hash'\n",
    "DBCACHELOCATION = '/tmp/database.cache'\n",
    "DATABASE_TABLE_NAME = os.environ.get('DATABASE_TABLE_NAME')\n",
    "INVENTORY_BUCKET_NAME = os.environ.get('INVENTORY_BUCKET_NAME')\n",
    "INPUT_BUCKET_NAME = os.environ.get('INPUT_BUCKET_NAME')\n",
    "REGION = os.environ.get('REGION') or 'ap-southeast-1'\n",
    "ACCESS_KEY_ID = os.environ.get('USER') or None\n",
    "SECRET_ACCESS_KEY = os.environ.get('PW') or None\n",
    "LINEKEY= os.environ.get('LINEKEY')\n",
    "  \n",
    "try:\n",
    "  DAX_ENDPOINT = os.environ['DAX_ENDPOINT']\n",
    "  print(DAX_ENDPOINT)\n",
    "except KeyError as e:\n",
    "  print(f'dax endpoint missing {e}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class S3Cache:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester( S3Cache, ProductDatabase):\n",
    "  class Meta:\n",
    "    table_name = os.environ['DATABASE_TABLE_NAME']\n",
    "    region = os.environ['REGION']\n",
    "    billing_mode='PAY_PER_REQUEST'\n",
    "    dax_read_endpoints = [DAX_ENDPOINT] if DAX_ENDPOINT else None\n",
    "    dax_write_endpoints = [DAX_ENDPOINT] if DAX_ENDPOINT else None\n",
    "  pass\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3Cache)\n",
    "def saveHash(cls , data:dict, key='allData', bucket=INVENTORY_BUCKET_NAME, \n",
    "             cachePath=DBCACHELOCATION, hashPath = DBHASHLOCATION):\n",
    "  hashKey = f'{key}-hash'\n",
    "  hashString = hashDict(data)\n",
    "  dictToSave= {'hash': hashString }\n",
    "  print(f'hashKey is {hashKey}')\n",
    "  print(f'saving cache file')\n",
    "  saveDictToFile(data, path = cachePath)\n",
    "  print(f'saving hash file')\n",
    "  saveStringToFile(hashString, path=hashPath)\n",
    "  print('saving hash to s3')\n",
    "  S3.save(key=hashKey,objectToSave=dictToSave, bucket=bucket)\n",
    "@add_class_method(S3Cache)\n",
    "def loadHash(cls,key='allData', bucket=INVENTORY_BUCKET_NAME):\n",
    "  hashKey = f'{key}-hash'\n",
    "  print(f'hashKey is {hashKey}')\n",
    "  loadedHash= S3.load(hashKey,bucket=bucket).get('hash')\n",
    "  return loadedHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:data was saved to s3\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashKey is testhash-hash\n",
      "saving cache file\n",
      "saving hash file\n",
      "saving hash to s3\n",
      "hashKey is testhash-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'h3aBJSFv1xUq9jXtp3bYCXksQYA='"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3Cache.saveHash({'test':'test'}, key='testhash')\n",
    "S3Cache.loadHash(key='testhash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3Cache)\n",
    "def loadFromS3(cls, bucketName= INVENTORY_BUCKET_NAME, key = 'allData',\n",
    "               hashPath=DBHASHLOCATION, cachePath = DBCACHELOCATION,**kwargs):\n",
    "  '''\n",
    "  this is not a real time function, there may be a delay of sync between\n",
    "  the main dynamodb database and the cache\n",
    "  '''\n",
    "  \n",
    "  if os.path.exists(hashPath) and os.path.exists(cachePath):\n",
    "    print('cache exist')\n",
    "    if cls.loadHash(key=key) == loadStringFromFile(hashPath):\n",
    "      db = loadDictFromFile(cachePath)\n",
    "      return db\n",
    "    else:\n",
    "      print('cache has different hash than s3')\n",
    "  print('cache doesnt exist')\n",
    "  logging.info(f'loading from {bucketName}')\n",
    "  logging.info(f'user is {kwargs.get(\"user\")}')\n",
    "  \n",
    "  return S3.loadPklZl(key=f'{key}-pklzl', bucket = bucketName,  **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:object doesnt exist\n",
      "INFO:root:loading from product-bucket-dev-manual\n",
      "INFO:root:user is None\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache exist\n",
      "hashKey is allData-hash\n",
      "cache has different hash than s3\n",
      "cache doesnt exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 678 ms, sys: 137 ms, total: 815 ms\n",
      "Wall time: 1.48 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0217153'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "list(Tester.loadFromS3().keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to s3 with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3Cache)\n",
    "def saveAllS3(cls, objectToSave:dict, bucketName= INVENTORY_BUCKET_NAME, key = 'allData', \n",
    "              hashPath = DBHASHLOCATION, cachePath = DBCACHELOCATION, **kwargs):\n",
    "  if os.path.exists(cachePath) and os.path.exists(hashPath):\n",
    "    if loadStringFromFile(hashPath) == cls.loadHash():\n",
    "      print('the object did not change, skip saving')\n",
    "      return\n",
    "  S3.save(key=key, bucket=bucketName, objectToSave=objectToSave)\n",
    "  S3.savePklZl(key=f'{key}-pklzl',bucket=bucketName, objectToSave=objectToSave)\n",
    "  S3.saveZl(key=f'{key}-zl',bucket=bucketName, objectToSave=objectToSave)\n",
    "  cls.saveHash(objectToSave, key=key)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:object doesnt exist\n",
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:data was saved to s3\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashKey is allData-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:data was saved to s3\n",
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:data was saved to s3\n",
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:data was saved to s3\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashKey is testKey-hash\n",
      "saving cache file\n",
      "saving hash file\n",
      "saving hash to s3\n",
      "cache exist\n",
      "hashKey is testKey-hash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test': 'test'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 'testKey'\n",
    "Tester.saveAllS3(objectToSave={'test':'test'}, key = key)\n",
    "Tester.loadFromS3(key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading from product-bucket-dev-manual\n",
      "INFO:root:user is None\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache doesnt exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:object exists, loading\n",
      "INFO:root:using accelerate endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 620 ms, sys: 140 ms, total: 761 ms\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "database = Tester.loadFromS3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621536"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(database )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 660 ms, sys: 113 ms, total: 773 ms\n",
      "Wall time: 768 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4dwjGB4O6LKgC8b5VZIWQi1hElU='"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nicHelper.dictUtil import hashDict\n",
    "%time hashDict(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using accelerate endpoint\n",
      "INFO:root:data was saved to s3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3.save(key='test',objectToSave=hashDict(database), bucket=INVENTORY_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
