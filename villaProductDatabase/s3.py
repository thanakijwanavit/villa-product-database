# AUTOGENERATED! DO NOT EDIT! File to edit: S3.ipynb (unless otherwise specified).

__all__ = ['DBHASHLOCATION', 'DBCACHELOCATION', 'DATABASE_TABLE_NAME', 'INVENTORY_BUCKET_NAME', 'INPUT_BUCKET_NAME',
           'REGION', 'ACCESS_KEY_ID', 'SECRET_ACCESS_KEY', 'LINEKEY', 'ALLDATAKEY', 'S3Cache', 'loadFromCache',
           'saveRemoteCache']

# Cell
from s3bz.s3bz import S3
from nicHelper.wrappers import add_method, add_class_method, add_static_method
from nicHelper.dictUtil import stripDict, printDict, hashDict, saveStringToFile, loadStringFromFile, saveDictToFile, loadDictFromFile
from nicHelper.exception import errorString
import nicHelper.pdUtils as pdUtils
from dict_hash import dict_hash, sha256
from base64 import b64encode, b64decode
from pandas.util import hash_pandas_object
from hashlib import sha1
import pandas as pd
import os, logging

# Cell
import os
DBHASHLOCATION = '/mnt/efs/database.hash'
DBCACHELOCATION = '/mnt/efs/database.cache'
DATABASE_TABLE_NAME = os.environ.get('DATABASE_TABLE_NAME')
INVENTORY_BUCKET_NAME = os.environ.get('INVENTORY_BUCKET_NAME')
INPUT_BUCKET_NAME = os.environ.get('INPUT_BUCKET_NAME')
REGION = os.environ.get('REGION') or 'ap-southeast-1'
ACCESS_KEY_ID = os.environ.get('USER') or None
SECRET_ACCESS_KEY = os.environ.get('PW') or None
LINEKEY= os.environ.get('LINEKEY')
ALLDATAKEY = 'allData'

try:
  DAX_ENDPOINT = os.environ['DAX_ENDPOINT']
  print(DAX_ENDPOINT)
except KeyError as e:
  print(f'dax endpoint missing {e}')


# Cell
class S3Cache:
  pass

# Cell
@add_class_method(S3Cache)
def loadFromCache(cls, key=ALLDATAKEY, localCache=DBCACHELOCATION,
                  localHash=DBHASHLOCATION, bucket=INVENTORY_BUCKET_NAME):
  ## check for local object and its hash
  if os.path.exists(localCache) and os.path.exists(localHash):
    localHash = pdUtils.loadLocalHash(path=localHash)
    remoteHash = pdUtils.loadRemoteHash(key=key, bucket=bucket)
    if localHash == remoteHash:
      print('data is still in sync, using local cache')
      db = pdUtils.loadLocalCache(path=localCache)
      return db
    else:
      print('remote hash is not the same, load remote cache')
  ### load from remote cache
  db = pdUtils.loadRemoteCache(key=key, bucket=bucket)
  ### save to local cache
  pdUtils.saveLocalCache(db, path=localCache)
  pdUtils.saveLocalHash(db, path = localHash)
  return db

# Cell

@add_class_method(S3Cache)
def saveRemoteCache(cls ,db:pd.DataFrame, key= ALLDATAKEY,
                   bucket = INVENTORY_BUCKET_NAME, localCachePath=DBCACHELOCATION,
                   localHashPath=DBHASHLOCATION):
  pdUtils.saveRemoteCache(data=db, key= key,
                          bucket=bucket, localCachePath=localCachePath,
                          localHashPath=localHashPath)
